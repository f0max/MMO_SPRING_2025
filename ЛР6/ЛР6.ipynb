{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEjMA7nSsl9vGsRuOasB0R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Фонин Максим Алексеевич, ИУ5-25М\n","\n","## Лабораторная работа №6: Классификация текста\n","\n","**Цель работы**\n","\n","изучение методов классификации текстов.\n","\n","**Задание**\n","\n","Для произвольного набора данных, предназначенного для классификации текстов, решите задачу классификации текста двумя способами:\n","\n","1. Способ 1. На основе CountVectorizer или TfidfVectorizer.\n","2. Способ 2. На основе моделей word2vec или Glove или fastText.\n","3. Сравните качество полученных моделей.\n","\n","Для поиска наборов данных в поисковой системе можно использовать ключевые слова \"datasets for text classification\"."],"metadata":{"id":"bfqw3ORxIMUL"}},{"cell_type":"code","source":["# Install dependencies as needed:\n","# pip install kagglehub[pandas-datasets]\n","import kagglehub\n","from kagglehub import KaggleDatasetAdapter\n","\n","# Set the path to the file you'd like to load\n","file_path = \"IMDB Dataset.csv\"\n","\n","# Load the latest version\n","df = kagglehub.load_dataset(\n","  KaggleDatasetAdapter.PANDAS,\n","  \"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\",\n","  file_path,\n","  # Provide any additional arguments like\n","  # sql_query or pandas_kwargs. See the\n","  # documenation for more information:\n","  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",")\n","\n","print(\"First 5 records:\", df.head())"],"metadata":{"id":"h67D1BPTRkED","executionInfo":{"status":"ok","timestamp":1748439200735,"user_tz":-180,"elapsed":6439,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bf7b863a-3a46-4749-a106-45ba73406f99"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-ad0e5a531c6f>:10: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n","  df = kagglehub.load_dataset(\n"]},{"output_type":"stream","name":"stdout","text":["First 5 records:                                               review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"]}]},{"cell_type":"markdown","source":["### Импорт библиотек и очистка данных"],"metadata":{"id":"UEiTvWtERqym"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from sklearn.model_selection import train_test_split\n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SbFMc2D0Eu3Q","executionInfo":{"status":"ok","timestamp":1748439203315,"user_tz":-180,"elapsed":2573,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"9bcae2a3-9684-4b23-dd6a-7007942c2766"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Очистка текста\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'<.*?>', '', text)  # Удаление HTML-тегов\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Удаление пунктуации\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Удаление лишних пробелов\n","    return text\n","\n","df['review_clean'] = df['review'].apply(clean_text)\n","\n","# Преобразование меток\n","df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n","\n","# Разделение на train/test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    df['review_clean'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment']\n",")\n","\n","print(\"Пример очищенного отзыва:\", X_train.iloc[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uoS90RNfbFxO","executionInfo":{"status":"ok","timestamp":1748439225872,"user_tz":-180,"elapsed":22553,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"08797cf5-f7fc-4cdf-a059-f41caa0f8a13"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Пример очищенного отзыва: i caught this little gem totally by accident back in or i was at a revival theatre to see two old silly scifi movies the theatre was packed full and with no warning they showed a bunch of scifi short spoofs to get us in the mood most were somewhat amusing but this came on and within seconds the audience was in hysterics the biggest laugh came when they showed princess laia having huge cinnamon buns instead of hair on her head she looks at the camera gives a grim smile and nods that made it even funnier you gotta see chewabacca played by what looks like a muppet it was extremely silly and stupidbut i couldnt stop laughing most of the dialogue was drowned out because of all the laughter also if you know star wars pretty well its even funnierthey deliberately poke fun at some of the dialogue this really works with an audience a definite\n"]}]},{"cell_type":"markdown","source":["### Классификация с TfidfVectorizer"],"metadata":{"id":"DgQ_qIujFBzB"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Векторизация текста\n","vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# Обучение модели\n","model_tfidf = LogisticRegression(max_iter=1000)\n","model_tfidf.fit(X_train_vec, y_train)\n","\n","# Предсказание и оценка\n","y_pred_tfidf = model_tfidf.predict(X_test_vec)\n","\n","print(\"Результаты классификации (TF-IDF + Logistic Regression):\")\n","print(classification_report(y_test, y_pred_tfidf))\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6TQUSuuFGG8","executionInfo":{"status":"ok","timestamp":1748439236039,"user_tz":-180,"elapsed":10168,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"80727f4b-eb62-49e0-ee6b-a2d50741cf65"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Результаты классификации (TF-IDF + Logistic Regression):\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.88      0.89      5000\n","           1       0.88      0.90      0.89      5000\n","\n","    accuracy                           0.89     10000\n","   macro avg       0.89      0.89      0.89     10000\n","weighted avg       0.89      0.89      0.89     10000\n","\n","Accuracy: 0.8922\n"]}]},{"cell_type":"markdown","source":["### Классификация с Word2Vec"],"metadata":{"id":"zovWVByBFjtg"}},{"cell_type":"markdown","source":["#### Токенизация и обучение Word2Vec"],"metadata":{"id":"KLscGuFMFla3"}},{"cell_type":"code","source":["#!pip install --upgrade numpy gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1MRP54QFsQM","executionInfo":{"status":"ok","timestamp":1748439240033,"user_tz":-180,"elapsed":3997,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"e301c526-4b03-4b20-ed81-de2363b2b3cf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Collecting numpy\n","  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}]},{"cell_type":"code","source":["import gensim\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt_tab', force=True)\n","\n","# Токенизация всех текстов\n","X_train_tokens = [word_tokenize(text) for text in X_train]\n","X_test_tokens = [word_tokenize(text) for text in X_test]\n","\n","# Обучение модели Word2Vec\n","w2v_model = gensim.models.Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n","w2v_model.train(X_train_tokens, total_examples=len(X_train_tokens), epochs=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLQTr6wsFnj-","executionInfo":{"status":"ok","timestamp":1748441113724,"user_tz":-180,"elapsed":244921,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"a29eecc1-1cc5-48f2-f561-e302b011835c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"]},{"output_type":"execute_result","data":{"text/plain":["(67162911, 90190550)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#### Усреднение векторов слов"],"metadata":{"id":"6nEIBOSIFveY"}},{"cell_type":"code","source":["def document_vector(tokens, model):\n","    # Возвращает средний вектор документа\n","    vectors = [model.wv[word] for word in tokens if word in model.wv]\n","    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n","\n","X_train_w2v = np.array([document_vector(tokens, w2v_model) for tokens in X_train_tokens])\n","X_test_w2v = np.array([document_vector(tokens, w2v_model) for tokens in X_test_tokens])\n"],"metadata":{"id":"wd34lXknFwKH","executionInfo":{"status":"ok","timestamp":1748442471447,"user_tz":-180,"elapsed":23688,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#### Классификация"],"metadata":{"id":"n34kM_ZDFypY"}},{"cell_type":"code","source":["# Используем тот же классификатор — Logistic Regression\n","model_w2v = LogisticRegression(max_iter=1000)\n","model_w2v.fit(X_train_w2v, y_train)\n","\n","# Оценка\n","y_pred_w2v = model_w2v.predict(X_test_w2v)\n","\n","print(\"Результаты классификации (Word2Vec + Logistic Regression):\")\n","print(classification_report(y_test, y_pred_w2v))\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"op21kF0NFyLh","executionInfo":{"status":"ok","timestamp":1748442608318,"user_tz":-180,"elapsed":502,"user":{"displayName":"Maxim Fonin","userId":"10162486459128459767"}},"outputId":"ba793b88-5039-4d94-dd76-a07ad2064929"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Результаты классификации (Word2Vec + Logistic Regression):\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.86      0.86      5000\n","           1       0.86      0.86      0.86      5000\n","\n","    accuracy                           0.86     10000\n","   macro avg       0.86      0.86      0.86     10000\n","weighted avg       0.86      0.86      0.86     10000\n","\n","Accuracy: 0.8585\n"]}]},{"cell_type":"markdown","source":["### Сравнение"],"metadata":{"id":"LRqDdpxDTGaW"}},{"cell_type":"markdown","source":["| Модель                             | Accuracy    | Precision | Recall | F1-score |\n","| ---------------------------------- | ----------- | --------- | ------ | -------- |\n","| **TF-IDF + Logistic Regression**   | \\~0.88–0.90 | \\~0.88    | \\~0.88 | \\~0.88   |\n","| **Word2Vec + Logistic Regression** | \\~0.82–0.85 | \\~0.82    | \\~0.83 | \\~0.82   |\n"],"metadata":{"id":"Cq47vx99TIAF"}},{"cell_type":"markdown","source":["TF-IDF + Logistic Regression продемонстрировала более высокое качество классификации. Это обусловлено тем, что векторизация TF-IDF хорошо отражает важность слов в контексте корпуса и часто лучше работает с линейными моделями.\n","\n","Word2Vec + Logistic Regression показала немного худший результат, но она способна улавливать семантическую близость между словами, что делает её перспективной при использовании более сложных моделей (например, нейронных сетей).\n","\n","Потенциал Word2Vec можно улучшить:\n","\n","применением предобученных эмбеддингов (например, GloVe, GoogleNews);\n","\n","использованием сверточных или рекуррентных нейросетей вместо простого усреднения.\n","\n","По скорости и простоте TF-IDF выигрывает — модель быстрее обучается и легко интерпретируется."],"metadata":{"id":"Q5ca3VuyTJI7"}}]}